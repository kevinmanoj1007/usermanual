"use strict";(self.webpackChunkuser_manual=self.webpackChunkuser_manual||[]).push([[3649],{5580:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Basics/optimization-loop","title":"Optimization Loop","description":"What is the optimization loop?","source":"@site/adk/Basics/optimization-loop.md","sourceDirName":"Basics","slug":"/Basics/optimization-loop","permalink":"/adk/Basics/optimization-loop","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Environment Data","permalink":"/adk/Basics/environment-data"},"next":{"title":"Agent Data","permalink":"/adk/Basics/agent-data"}}');var r=i(4848),s=i(8453);const o={sidebar_position:8},a="Optimization Loop",d={},c=[{value:"What is the optimization loop?",id:"what-is-the-optimization-loop",level:2},{value:"Key Components",id:"key-components",level:2},{value:"Optimization Loop Flow",id:"optimization-loop-flow",level:2},{value:"Preprocessing Integration",id:"preprocessing-integration",level:2},{value:"Model Persistence",id:"model-persistence",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"optimization-loop",children:"Optimization Loop"})}),"\n",(0,r.jsx)(n.h2,{id:"what-is-the-optimization-loop",children:"What is the optimization loop?"}),"\n",(0,r.jsx)(n.p,{children:"The optimization loop is a loop that is continuously run until:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"An agent is finished learning."}),"\n",(0,r.jsx)(n.li,{children:"A learnt agent is finished optimizing a system."}),"\n",(0,r.jsx)(n.li,{children:'Optimization / training is manually stopped midway through optimization / training with "Stop Optimization".'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'The optimization loop is always started with "Start Optimization" on the web interface\'s "Genie" in a project.'}),"\n",(0,r.jsx)(n.h2,{id:"key-components",children:"Key Components"}),"\n",(0,r.jsx)(n.p,{children:"The optimization loop involves two primary components:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"agent"})}),": The reinforcement learning agent that learns to optimize the system. It has methods like:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"agent.step(observation, info)"}),": Computes the action based on current state"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"agent.sample_env_parameters()"}),": Samples environment parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"agent.add_experiences(...)"}),": Stores transition data for learning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"agent.learn()"}),": Updates the agent's policy based on experiences"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"agent.save_models()"}),": Saves the current model weights"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"agent.load_models()"}),": Loads previously saved model weights"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"agent.env"})}),": The environment representing the system being optimized. It has methods like:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"agent.env.reset()"}),": Resets the environment to initial state, returns ",(0,r.jsx)(n.code,{children:"observation"})," and ",(0,r.jsx)(n.code,{children:"info"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"agent.env.step(step_data)"}),": Executes an action in the environment, returns ",(0,r.jsx)(n.code,{children:"next_observation"}),", ",(0,r.jsx)(n.code,{children:"reward"}),", ",(0,r.jsx)(n.code,{children:"terminated"}),", ",(0,r.jsx)(n.code,{children:"truncated"}),", and ",(0,r.jsx)(n.code,{children:"next_info"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"optimization-loop-flow",children:"Optimization Loop Flow"}),"\n",(0,r.jsx)(n.p,{children:"The optimization loop broadly follows this sequence in the ADK's executor routine:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# 1. Setup phase\r\nspecification = get_optimization_specification_from_platform()\r\nspecification = process_specification(specification)  # Preprocess specifications\r\n\r\nagent = initialize_agent(specification)\r\nenv = agent.env\r\n\r\n# 2. Environment initialization\r\nobservation, info = env.reset()\r\nobservation, info = process_env_reset(observation, info)  # Preprocess initial state\r\n\r\n# 3. Load models if running in inference mode\r\nif specification.mode == "inference":\r\n    agent.load_models()\r\n\r\n# 4. Main optimization loop\r\nbest_episode_reward = -float(\'inf\')\r\nepisode_reward = 0\r\n\r\nwhile optimizing:\r\n    # Compute action from agent\r\n    action = agent.step(observation, info)\r\n    \r\n    # Sample environment parameters\r\n    env_parameters = agent.sample_env_parameters()\r\n    \r\n    # Construct and preprocess step data\r\n    step_data = construct_step_data(action, env_parameters)\r\n    step_data = process_agent_step_data(step_data)  # Preprocess step data\r\n    \r\n    # Execute step in environment\r\n    next_observation, reward, terminated, truncated, next_info = env.step(step_data)\r\n    \r\n    # Preprocess environment response\r\n    next_observation, reward, terminated, truncated, next_info = process_env_step(\r\n        next_observation, reward, terminated, truncated, next_info\r\n    )\r\n    \r\n    # Store experience for learning (training mode only)\r\n    if specification.mode == "training":\r\n        agent.add_experiences(\r\n            observation, action, reward, next_observation, \r\n            terminated, truncated, info, next_info\r\n        )\r\n        \r\n        # Learn from experiences\r\n        agent.learn()\r\n    \r\n    # Accumulate episode reward\r\n    episode_reward += reward\r\n    \r\n    # Update current state\r\n    observation = next_observation\r\n    info = next_info\r\n    \r\n    # Handle episode termination\r\n    if terminated or truncated:\r\n        # In inference mode, stop after first episode completion\r\n        if specification.mode == "inference" and terminated:\r\n            break\r\n        \r\n        # In training mode, save models if this episode is the best so far\r\n        if specification.mode == "training" and episode_reward > best_episode_reward:\r\n            agent.save_models()\r\n            best_episode_reward = episode_reward\r\n        \r\n        # Reset environment for next episode\r\n        observation, info = env.reset()\r\n        observation, info = process_env_reset(observation, info)  # Preprocess reset state\r\n        episode_reward = 0\r\n\r\n# 5. Cleanup\r\nfinalize_optimization()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"preprocessing-integration",children:"Preprocessing Integration"}),"\n",(0,r.jsx)(n.p,{children:"Preprocessing is integrated throughout the optimization loop to ensure data consistency and proper formatting:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Specification preprocessing"})," (",(0,r.jsx)(n.code,{children:"process_specification"}),")"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Applied once at the start to validate and transform optimization specifications"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Environment reset preprocessing"})," (",(0,r.jsx)(n.code,{children:"process_env_reset"}),")"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Applied after each ",(0,r.jsx)(n.code,{children:"env.reset()"})," call"]}),"\n",(0,r.jsx)(n.li,{children:"Transforms initial observations and info before agent processes them"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step data preprocessing"})," (",(0,r.jsx)(n.code,{children:"process_agent_step_data"}),")"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Applied before each ",(0,r.jsx)(n.code,{children:"env.step()"})," call"]}),"\n",(0,r.jsx)(n.li,{children:"Ensures action and environment parameters are properly formatted"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Environment step preprocessing"})," (",(0,r.jsx)(n.code,{children:"process_env_step"}),")"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Applied after each ",(0,r.jsx)(n.code,{children:"env.step()"})," call"]}),"\n",(0,r.jsx)(n.li,{children:"Transforms observations, rewards, and info before they're used by the agent"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"model-persistence",children:"Model Persistence"}),"\n",(0,r.jsx)(n.p,{children:"Models are loaded and saved at specific points in the optimization loop:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loading"}),": Models are loaded once at the beginning if running in inference mode"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Saving"}),": Models are saved during training whenever an episode achieves a new best reward, ensuring that only improved versions are persisted"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);