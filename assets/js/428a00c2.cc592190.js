"use strict";(self.webpackChunkuser_manual=self.webpackChunkuser_manual||[]).push([[51],{1348:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"API/Environments/direct-action-env","title":"DirectActionEnv","description":"A default environment implementation provided with the ADK. Directly sets the world controls to the","source":"@site/adk/API/Environments/direct-action-env.md","sourceDirName":"API/Environments","slug":"/API/Environments/direct-action-env","permalink":"/adk/API/Environments/direct-action-env","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"DirectActionEnv"},"sidebar":"tutorialSidebar","previous":{"title":"OptimizationEnv","permalink":"/adk/API/Environments/optimization-env"},"next":{"title":"ClippedAdditiveActionEnv","permalink":"/adk/API/Environments/additive-action-env"}}');var r=i(4848),t=i(8453);const l={sidebar_position:2,title:"DirectActionEnv"},o="class DirectActionEnv",a={},d=[{value:"Import",id:"import",level:2},{value:"Members",id:"members",level:2},{value:"<code>reward_scaling_factors: ArrayLike | None</code>:",id:"reward_scaling_factors-arraylike--none",level:3},{value:"<code>all_targets_not_satisfied_reward: float</code>",id:"all_targets_not_satisfied_reward-float",level:3},{value:"<code>failed_observation_evaluation_reward: float</code>",id:"failed_observation_evaluation_reward-float",level:3},{value:"<code>all_targets_satisfied_reward: float</code>",id:"all_targets_satisfied_reward-float",level:3},{value:"<code>observation_space: gymnasium.spaces.Box</code>",id:"observation_space-gymnasiumspacesbox",level:3},{value:"<code>action_space: gymnasium.spaces.Box</code>",id:"action_space-gymnasiumspacesbox",level:3},{value:"Methods",id:"methods",level:2},{value:"step",id:"step",level:3},{value:"reset",id:"reset",level:3},{value:"render",id:"render",level:3}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"class-directactionenv",children:"class DirectActionEnv"})}),"\n",(0,r.jsx)(n.p,{children:"A default environment implementation provided with the ADK. Directly sets the world controls to the\r\nprovided action every step. Each episode either terminates or truncates on the first step."}),"\n",(0,r.jsx)(n.h2,{id:"import",children:"Import"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'# The env is registered with Gymnasium in the ADK. No need to specially import.\r\nimport adk\r\nimport gymnasium\r\n\r\n# Make env.\r\nenv = gymnasium.make("AI4EE-Direct-Action-Env")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"members",children:"Members"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.h3,{id:"reward_scaling_factors-arraylike--none",children:[(0,r.jsx)(n.code,{children:"reward_scaling_factors: ArrayLike | None"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": An array of scaling factors to multiply each sub-reward by for each evaluated\r\nobservation."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"all_targets_not_satisfied_reward-float",children:(0,r.jsx)(n.code,{children:"all_targets_not_satisfied_reward: float"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The reward added to the total reward for a step when all the targets are\r\nnot satisfied on that step."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"failed_observation_evaluation_reward-float",children:(0,r.jsx)(n.code,{children:"failed_observation_evaluation_reward: float"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The reward added to the total reward for a step when evaluating an observation\r\nfails on that step."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"all_targets_satisfied_reward-float",children:(0,r.jsx)(n.code,{children:"all_targets_satisfied_reward: float"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The reward added to the total reward for a step when all targets are satisfied\r\non that step."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"observation_space-gymnasiumspacesbox",children:(0,r.jsx)(n.code,{children:"observation_space: gymnasium.spaces.Box"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The observation space of the environment. Equivalent to the\r\nconcatenation of the following in order:"]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"world_observation_bounds"}),": The evaluated observations after taking a given action."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"world_control_bounds"}),": The given action evaluating to the aforementioned observations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"world_observation_bounds"}),": The equivalence targets corresponding to each evaluated observation."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"action_space-gymnasiumspacesbox",children:(0,r.jsx)(n.code,{children:"action_space: gymnasium.spaces.Box"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The action space of the environment. Equivalent to the ",(0,r.jsx)(n.code,{children:"world_control_space"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"methods",children:"Methods"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"step",children:"step"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": Takes a step in the environment with a direct action and stochastic parameters.\r\nThe world controls and stochastic parameters are directly set to the provided action and parameters,\r\nthen the system is run."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Takes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"step_data: StepData"}),": The step data with which to run the system."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Returns (as a tuple)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"observations: numpy.typing.NDArray[numpy.float32]"}),": The observations in the same form\r\nas described in the ",(0,r.jsx)(n.code,{children:"observation_space"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"reward: float"}),": The reward for the given action. Calculated as through:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"-1.0 * sum(\r\n    (abs(Equivalence Targets - Evaluated observations) * Reward Scaling Factors) ^ 2\r\n) + (\r\n    All Targets Satisfied Reward (if applicable only) +\r\n    All Targets Not Satisfied Reward (if applicable only)\r\n)\n"})}),"\n","in the case that observation evaluation fails, it is calculated using:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"(\r\n    All Targets Not Satisfied Reward +\r\n    Failed Observation Evaluation Reward\r\n)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"terminated: bool"}),": Whether or not all the targets were ",(0,r.jsx)(n.strong,{children:"satisfied"})," by the given action."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"truncated: bool"}),": Whether or not all the targets were ",(0,r.jsx)(n.strong,{children:"NOT satisfied"})," by the given action.\r\nNecessarily ",(0,r.jsx)(n.strong,{children:"true"})," if ",(0,r.jsx)(n.code,{children:"terminated"})," is ",(0,r.jsx)(n.strong,{children:"false"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"info: dict[str, typing.Any]"}),": No auxiliary information provided. Always an empty dictionary."]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["Since ",(0,r.jsx)(n.code,{children:"truncated"})," is guaranteed to be true whenever ",(0,r.jsx)(n.code,{children:"terminated"})," is false, this indeed\r\nmeans each episode is only 1 step. This is intended as being able to directly set the world\r\ncontrols means there are no future consequences to bad actions, therefore, not requiring\r\niterative optimization over multiple steps."]})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"reset",children:"reset"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": Resets the env. Initially uses randomly sampled world controls."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Takes"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"seed: int | None"}),": The seed for random sampling. Ignored even if provided."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"options: dict[str, typing.Any] | None"}),": Additional information for reset. Ignored even\r\nif provided."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns (as a tuple)"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"observations: numpy.typing.NDArray[numpy.float32]"}),": The observations in the same form\r\nas described in the ",(0,r.jsx)(n.code,{children:"observation_space"}),". Constructed with random world controls."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"info: dict[str, typing.Any]"}),": No auxiliary information provided. Always an empty dictionary."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"render",children:"render"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": Render the environment state. Currently unsupported. All calls to this method\r\nare ignored."]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Takes: Nothing"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"rendered_frame: None"}),": Just a ",(0,r.jsx)(n.code,{children:"None"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);