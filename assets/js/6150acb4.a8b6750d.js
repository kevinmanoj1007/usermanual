"use strict";(self.webpackChunkuser_manual=self.webpackChunkuser_manual||[]).push([[3345],{2129:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"API/Environments/additive-action-env","title":"ClippedAdditiveActionEnv","description":"A default environment implementation provided with the ADK. Used to iteratively add the provided","source":"@site/adk/API/Environments/additive-action-env.md","sourceDirName":"API/Environments","slug":"/API/Environments/additive-action-env","permalink":"/adk/API/Environments/additive-action-env","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"ClippedAdditiveActionEnv"},"sidebar":"tutorialSidebar","previous":{"title":"DirectActionEnv","permalink":"/adk/API/Environments/direct-action-env"},"next":{"title":"Models","permalink":"/adk/category/models"}}');var r=i(4848),t=i(8453);const l={sidebar_position:3,title:"ClippedAdditiveActionEnv"},d="class ClippedAdditiveActionEnv",o={},a=[{value:"Import",id:"import",level:2},{value:"Members",id:"members",level:2},{value:"<code>reward_scaling_factors: ArrayLike | None</code>:",id:"reward_scaling_factors-arraylike--none",level:3},{value:"<code>n_action_intervals: int</code>:",id:"n_action_intervals-int",level:3},{value:"<code>episode_maximum_steps: int</code>:",id:"episode_maximum_steps-int",level:3},{value:"<code>episode_truncation_reward: float</code>",id:"episode_truncation_reward-float",level:3},{value:"<code>failed_observation_evaluation_reward: float</code>",id:"failed_observation_evaluation_reward-float",level:3},{value:"<code>all_targets_satisfied_reward: float</code>",id:"all_targets_satisfied_reward-float",level:3},{value:"<code>target_generators: list[Callable[[float], list[float]]] | None</code>",id:"target_generators-listcallablefloat-listfloat--none",level:3},{value:"<code>observation_space: gymnasium.spaces.Box</code>",id:"observation_space-gymnasiumspacesbox",level:3},{value:"<code>action_space: gymnasium.spaces.Box</code>",id:"action_space-gymnasiumspacesbox",level:3},{value:"Methods",id:"methods",level:2},{value:"step",id:"step",level:3},{value:"reset",id:"reset",level:3},{value:"render",id:"render",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"class-clippedadditiveactionenv",children:"class ClippedAdditiveActionEnv"})}),"\n",(0,r.jsx)(n.p,{children:"A default environment implementation provided with the ADK. Used to iteratively add the provided\r\naction to the world controls every step. Each episode either terminates or truncates after a maximum\r\nnumber of steps OR after satisfying the provided targets."}),"\n",(0,r.jsx)(n.h2,{id:"import",children:"Import"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'# The env is registered with Gymnasium in the ADK. No need to specially import.\r\nimport adk\r\nimport gymnasium\r\n\r\n# Make env.\r\nenv = gymnasium.make("AI4EE-Clipped-Additive-Action-Env")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"members",children:"Members"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.h3,{id:"reward_scaling_factors-arraylike--none",children:[(0,r.jsx)(n.code,{children:"reward_scaling_factors: ArrayLike | None"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": An array of scaling factors to multiply each sub-reward by for each evaluated\r\nobservation."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.h3,{id:"n_action_intervals-int",children:[(0,r.jsx)(n.code,{children:"n_action_intervals: int"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The number of intervals to divide the world control space into. i.e. The right\r\nactions can go from any point in the action to any other point in the action space given that\r\na minimum of ",(0,r.jsx)(n.code,{children:"n_action_intervals"})," number of actions are taken."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.h3,{id:"episode_maximum_steps-int",children:[(0,r.jsx)(n.code,{children:"episode_maximum_steps: int"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The maximum number of steps that can be taken before the episode is terminated.\r\nIt is highly recommended that this is always set to a value that is larger than or equal to\r\nthe number of action intervals."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"episode_truncation_reward-float",children:(0,r.jsx)(n.code,{children:"episode_truncation_reward: float"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The reward added to the total reward on the step that the maximum number of\r\nallowed steps within an episode is exceeded."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"failed_observation_evaluation_reward-float",children:(0,r.jsx)(n.code,{children:"failed_observation_evaluation_reward: float"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The reward added to the total reward for a step when evaluating an observation\r\nfails on that step."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"all_targets_satisfied_reward-float",children:(0,r.jsx)(n.code,{children:"all_targets_satisfied_reward: float"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The reward added to the total reward for a step when all targets are satisfied\r\non that step."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"target_generators-listcallablefloat-listfloat--none",children:(0,r.jsx)(n.code,{children:"target_generators: list[Callable[[float], list[float]]] | None"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": A list of functions parameterized by a ",(0,r.jsx)(n.code,{children:"seed: float"}),", that return newly\r\ngenerated target values after every ",(0,r.jsx)(n.code,{children:"reset()"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"observation_space-gymnasiumspacesbox",children:(0,r.jsx)(n.code,{children:"observation_space: gymnasium.spaces.Box"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The observation space of the environment. Equivalent to the\r\nconcatenation of the following in order:"]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"low: -world_observation_bounds_diff, high: +world_observation_bounds_diff"}),": The difference\r\nbetween the evaluated observations and equivalence targets."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"world_control_bounds"}),": The world controls after adding and clipping the given action,\r\nwhich resulted in evaluating to the aforementioned observations."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"action_space-gymnasiumspacesbox",children:(0,r.jsx)(n.code,{children:"action_space: gymnasium.spaces.Box"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": The action space of the environment. Equivalent to the ",(0,r.jsx)(n.code,{children:"world_control_space"}),"\r\ndivided by the number of action intervals."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"methods",children:"Methods"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"step",children:"step"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": Takes a step in the environment with an additive action. The action is added\r\nto the world controls and the world controls are clipped to the bounds."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Takes"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"step_data: StepData"}),": The step data with which to run the system. The actions are additively\r\ncomputed with clipping, while the stochastic parameters are used directly as-is."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns (as a tuple)"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"observations: numpy.typing.NDArray[numpy.float32]"}),": The observations in the same form\r\nas described in the ",(0,r.jsx)(n.code,{children:"observation_space"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"reward: float"}),": The reward for the given action. Calculated as through:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"-1.0 * sum(\r\n    (abs(Equivalence Targets - Evaluated observations) * Reward Scaling Factors) ^ 2\r\n) + (\r\n    All Targets Satisfied Reward (if applicable only) +\r\n    Episode Truncation Reward (if applicable only) +\r\n    Failed Observation Evaluation Reward (if applicable only)\r\n)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"terminated: bool"}),": Whether or not all the targets were ",(0,r.jsx)(n.strong,{children:"satisfied"})," by the computed world controls."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"truncated: bool"}),": Whether or not all the targets were ",(0,r.jsx)(n.strong,{children:"NOT satisfied"})," by the computed world controls."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"info: dict[str, typing.Any]"}),": No auxiliary information provided. Always an empty dictionary."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"reset",children:"reset"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": Resets the env. Initially uses randomly sampled world controls."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Takes"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"seed: int | None"}),": The seed for random sampling. Ignored even if provided."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"options: dict[str, typing.Any] | None"}),": Additional information for reset. Ignored even\r\nif provided."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns (as a tuple)"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"observations: numpy.typing.NDArray[numpy.float32]"}),": The observations in the same form\r\nas described in the ",(0,r.jsx)(n.code,{children:"observation_space"}),". Constructed with random world controls."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"info: dict[str, typing.Any]"}),": No auxiliary information provided. Always an empty dictionary."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\xa0"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.h3,{id:"render",children:"render"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description"}),": Render the environment state. Currently unsupported. All calls to this method\r\nare ignored."]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Takes: Nothing"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"rendered_frame: None"}),": Just a ",(0,r.jsx)(n.code,{children:"None"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>d});var s=i(6540);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);